from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage
from ..state import BlogState
from ..config import settings
import json, re, os

if settings.google_api_key:
    os.environ["GOOGLE_API_KEY"] = settings.google_api_key

llm = ChatGoogleGenerativeAI(
    model=settings.gemini_model,
    temperature=0.3,
)


def research(state: BlogState) -> dict:
    """
    [Node 2] Tavilyë¡œ ì£¼ì œ ê´€ë ¨ ìµœì‹  ì •ë³´ ì›¹ ê²€ìƒ‰
    
    íë¦„:
    1. LLMì´ ì£¼ì œë¥¼ ë¶„ì„í•´ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ 3ê°œ ìƒì„±
    2. ê° ì¿¼ë¦¬ë¡œ Tavily ê²€ìƒ‰ ì‹¤í–‰
    3. ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ + ì°¸ê³  URL ì¶”ì¶œ
    """
    topic = state["topic"]

    # â”€â”€ Step 1: ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    query_prompt = f"""ë¸”ë¡œê·¸ ì£¼ì œ: "{topic}"

ì´ ì£¼ì œë¡œ ê¹Šì´ ìˆëŠ” ë¸”ë¡œê·¸ë¥¼ ì‘ì„±í•˜ê¸° ìœ„í•œ ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ 3ê°œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
- ì˜ì–´ ì¿¼ë¦¬ (ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆì´ ë” ë†’ìŒ)
- ê°ê° ë‹¤ë¥¸ ê°ë„ì—ì„œ ì ‘ê·¼ (ê°œìš”, ì‹¤ìš©ì  ì‚¬ë¡€, ìµœì‹  ë™í–¥)

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{"queries": ["query1", "query2", "query3"]}}"""

    response = llm.invoke([HumanMessage(content=query_prompt)])

    try:
        content = re.sub(r"```(?:json)?|```", "", response.content).strip()
        queries = json.loads(content).get("queries", [topic])
    except Exception:
        queries = [topic, f"{topic} tutorial", f"{topic} best practices"]

    # â”€â”€ Step 2: Tavily ê²€ìƒ‰ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    os.environ["TAVILY_API_KEY"] = settings.tavily_api_key

    search_tool = TavilySearchResults(max_results=3)
    raw_results = []
    references = []

    for query in queries[:3]:
        try:
            results = search_tool.invoke(query)
            for r in results:
                raw_results.append({
                    "query":   query,
                    "title":   r.get("title", ""),
                    "content": r.get("content", "")[:500],  # 500ì ì œí•œ
                    "url":     r.get("url", ""),
                })
                if r.get("url"):
                    references.append(r["url"])
        except Exception as e:
            raw_results.append({"query": query, "error": str(e)})

    # â”€â”€ Step 3: ê²°ê³¼ ìš”ì•½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    results_text = "\n\n".join([
        f"[ê²€ìƒ‰: {r['query']}]\nì œëª©: {r.get('title','')}\në‚´ìš©: {r.get('content','')}"
        for r in raw_results if "error" not in r
    ])

    summary_prompt = f"""ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ë¸”ë¡œê·¸ ì‘ì„±ì— í™œìš©í•  í•µì‹¬ ì •ë³´, í†µê³„, ì‚¬ë¡€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.

ê²€ìƒ‰ ê²°ê³¼:
{results_text[:3000]}

ìš”ì•½ (500ì ì´ë‚´):"""

    summary_response = llm.invoke([HumanMessage(content=summary_prompt)])

    return {
        "research_results": [summary_response.content.strip()],
        "references":       list(set(references)),  # ì¤‘ë³µ ì œê±°
        "logs":             [f"ğŸ” [Research] ì¿¼ë¦¬ {len(queries)}ê°œ, ê²°ê³¼ {len(raw_results)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ"],
    }
